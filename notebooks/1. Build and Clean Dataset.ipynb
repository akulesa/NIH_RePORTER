{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4feafd86",
   "metadata": {},
   "source": [
    "# Build and Clean Dataset\n",
    "\n",
    "\n",
    "This notebook cleans up and consolidates the NIH grant datasets. \n",
    "\n",
    "Inputs:\n",
    " - ../data/raw/ should contain csv datasets downloaded from NIH project exporter\n",
    "Outputs: \n",
    " - ../data/clean/NIH_grants.csv should be cleaned and consolidated dataset\n",
    " \n",
    " \n",
    "Steps:\n",
    "- For each abstract, convert all words to lowercase and remove alphanumeric characters\n",
    "- Lemmatize all words using en_core_web_sm model from gensim\n",
    "- Create bigram model and learn word pairs\n",
    "- Output single dataframe with all grant information and cleaned, lemmatized, bigrammed grant abstracts\n",
    " \n",
    "Bigram models will be saved to ../models/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84087a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541ec16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akulesa/opt/anaconda3/envs/word2vec/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy en_core_web_sm loaded!\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from grantminer.config import filename, filepath\n",
    "import grantminer.data as data\n",
    "import pandas as pd\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5afc8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up year: 2018\n",
      "Reading filename: ../data/raw/RePORTER_PRJABS_C_FY2018_new.csv\n",
      "Number of lines: 80395\n",
      "Lowercased and removed non-alphanumeric characters.\n",
      "Cleaned and lemmatized.\n",
      "Time to clean up: 38.08 mins\n",
      "Cleaning up year: 2019\n",
      "Reading filename: ../data/raw/RePORTER_PRJABS_C_FY2019_new.csv\n",
      "Number of lines: 79107\n",
      "Lowercased and removed non-alphanumeric characters.\n",
      "Cleaned and lemmatized.\n",
      "Time to clean up: 35.13 mins\n",
      "Cleaning up year: 2020\n",
      "Reading filename: ../data/raw/RePORTER_PRJABS_C_FY2020_new.csv\n",
      "Number of lines: 78028\n",
      "Lowercased and removed non-alphanumeric characters.\n",
      "Cleaned and lemmatized.\n",
      "Time to clean up: 32.42 mins\n"
     ]
    }
   ],
   "source": [
    "years = [2018, 2019, 2020]\n",
    "agg = []\n",
    "\n",
    "for y in years:\n",
    "    \n",
    "    print('Cleaning up year: '+str(y))\n",
    "    \n",
    "    t_ = time()\n",
    "    \n",
    "    d = data.load(y,abstract=True,verbose=True) \\\n",
    "        .pipe(data.clean_text_from_df,'ABSTRACT_TEXT',verbose=True) \\\n",
    "        .drop(labels=['ABSTRACT_TEXT'],axis=1) \\\n",
    "        .assign(Year=y)\n",
    "    \n",
    "    d.to_csv(filename(y,abstract=True,fpath='processed'))\n",
    "    \n",
    "    print('Time to clean up: {} mins'.format(round((time() - t_) / 60, 2)))\n",
    "    \n",
    "    agg.append(d)\n",
    "\n",
    "\n",
    "nih_abstracts = pd.concat(agg).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca089978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 55941: expected 46 fields, saw 47\\n'\n",
      "b'Skipping line 3706: expected 46 fields, saw 47\\n'\n",
      "b'Skipping line 63777: expected 46 fields, saw 47\\n'\n"
     ]
    }
   ],
   "source": [
    "application_fields = pd.concat([data.load(y) for y in years]).reset_index(drop=True)\n",
    "\n",
    "cleaned = application_fields.merge(nih_abstracts,on='APPLICATION_ID',how='right')\n",
    "cleaned.to_csv(filepath['processed'] / 'NIH_grants_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea0bafec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove blanks\n",
    "blanks = cleaned['clean'].apply(type)!=str\n",
    "blanks.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ab3b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned.drop(cleaned.index[blanks],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "854ff1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.to_csv(filepath['processed'] / 'NIH_grants_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a8c11af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:15:05: collecting all words and their counts\n",
      "INFO - 15:15:05: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 15:15:08: PROGRESS: at sentence #10000, processed 2379668 words and 1092174 word types\n",
      "INFO - 15:15:11: PROGRESS: at sentence #20000, processed 4718402 words and 1829551 word types\n",
      "INFO - 15:15:14: PROGRESS: at sentence #30000, processed 7115000 words and 2470831 word types\n",
      "INFO - 15:15:17: PROGRESS: at sentence #40000, processed 9498333 words and 3039229 word types\n",
      "INFO - 15:15:19: PROGRESS: at sentence #50000, processed 11833021 words and 3537151 word types\n",
      "INFO - 15:15:22: PROGRESS: at sentence #60000, processed 14159768 words and 3986345 word types\n",
      "INFO - 15:15:25: PROGRESS: at sentence #70000, processed 16493681 words and 4394584 word types\n",
      "INFO - 15:15:28: PROGRESS: at sentence #80000, processed 18825366 words and 4759227 word types\n",
      "INFO - 15:15:31: PROGRESS: at sentence #90000, processed 21242987 words and 4847630 word types\n",
      "INFO - 15:15:34: PROGRESS: at sentence #100000, processed 23650143 words and 4981434 word types\n",
      "INFO - 15:15:37: PROGRESS: at sentence #110000, processed 26063014 words and 5085560 word types\n",
      "INFO - 15:15:40: PROGRESS: at sentence #120000, processed 28429460 words and 5169061 word types\n",
      "INFO - 15:15:43: PROGRESS: at sentence #130000, processed 30792312 words and 5297683 word types\n",
      "INFO - 15:15:46: PROGRESS: at sentence #140000, processed 33161910 words and 5455564 word types\n",
      "INFO - 15:15:49: PROGRESS: at sentence #150000, processed 35529397 words and 5646709 word types\n",
      "INFO - 15:15:52: PROGRESS: at sentence #160000, processed 37924473 words and 5814759 word types\n",
      "INFO - 15:15:55: PROGRESS: at sentence #170000, processed 40333098 words and 5865992 word types\n",
      "INFO - 15:15:57: PROGRESS: at sentence #180000, processed 42715981 words and 6028529 word types\n",
      "INFO - 15:16:00: PROGRESS: at sentence #190000, processed 45087369 words and 6135511 word types\n",
      "INFO - 15:16:03: PROGRESS: at sentence #200000, processed 47486870 words and 6176863 word types\n",
      "INFO - 15:16:07: PROGRESS: at sentence #210000, processed 49886227 words and 6276726 word types\n",
      "INFO - 15:16:10: PROGRESS: at sentence #220000, processed 52255133 words and 6411829 word types\n",
      "INFO - 15:16:13: PROGRESS: at sentence #230000, processed 54567565 words and 6558817 word types\n",
      "INFO - 15:16:15: collected 6700854 token types (unigram + bigrams) from a corpus of 56196549 words and 237168 sentences\n",
      "INFO - 15:16:15: merged Phrases<6700854 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 15:16:15: Phrases lifecycle event {'msg': 'built Phrases<6700854 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 69.48s', 'datetime': '2021-05-04T15:16:15.182946', 'gensim': '4.0.1', 'python': '3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:12:38) \\n[Clang 11.0.1 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO - 15:16:16: Phrases lifecycle event {'fname_or_handle': '../models/bigram.pkl', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-05-04T15:16:16.336160', 'gensim': '4.0.1', 'python': '3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:12:38) \\n[Clang 11.0.1 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "INFO - 15:16:18: saved ../models/bigram.pkl\n"
     ]
    }
   ],
   "source": [
    "# Learn bigram model using Gensim Phraser\n",
    "bigram_model = data.find_bigrams(cleaned,'clean')\n",
    "bigram_model.save(str(filepath['bigram']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b80c19be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:16:18: exporting phrases from Phrases<6700854 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 15:16:31: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<19309 phrases, min_count=30, threshold=10.0> from Phrases<6700854 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 13.10s', 'datetime': '2021-05-04T15:16:31.644659', 'gensim': '4.0.1', 'python': '3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:12:38) \\n[Clang 11.0.1 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO - 15:16:31: FrozenPhrases lifecycle event {'fname_or_handle': '../models/bigram_frozen.pkl', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-05-04T15:16:31.974993', 'gensim': '4.0.1', 'python': '3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:12:38) \\n[Clang 11.0.1 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "INFO - 15:16:31: saved ../models/bigram_frozen.pkl\n"
     ]
    }
   ],
   "source": [
    "# Freeze the model into smaller more efficient version\n",
    "bigram_model = data.freeze_bigram(bigram_model)\n",
    "bigram_model.save(str(filepath['bigram_frozen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e142154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['clean'] = cleaned['clean'].apply(lambda x: ' '.join(bigram_model[x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3e838c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.reset_index(drop=True).to_csv(filepath['clean'] / 'NIH_grants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84d52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
